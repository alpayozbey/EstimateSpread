# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d0TYnm_5t9FSABRDu_c9EpFwonxq2B8E
"""

import re
import time
import datetime 
import operator
import numpy as np
import pandas as pd 
import collections
import unicodedata
import collections
import seaborn as sns
import collections
import matplotlib.pylab as pylab
import matplotlib.pyplot as plt

from tqdm import tqdm
from collections import Counter
from datetime import datetime, date, timedelta
from IPython.display import Image
from sklearn.ensemble import RandomForestRegressor

from statsmodels.tools import eval_measures

#Data reading
#Load train and test csv file
dengue_features_train = pd.read_csv('dengue_features_train.csv')
dengue_features_test = pd.read_csv('dengue_features_test.csv')
dengue_labels_train = pd.read_csv('dengue_labels_train.csv')
#Let's fusion the 2 dataframes:
#Merging the Train dataframe with the labels data frame 

dengue_train = pd.merge(dengue_labels_train, dengue_features_train, on=['city','year','weekofyear'])

dengue_train_sj = dengue_train[dengue_train.city == 'sj'].copy()
dengue_train_iq = dengue_train[dengue_train.city == 'iq'].copy()
dengue_test_sj = dengue_features_test[dengue_features_test.city == 'sj'].copy()
dengue_test_iq = dengue_features_test[dengue_features_test.city == 'iq'].copy()

#A few bad variables
#we can drop the columns with negative corellation in both 
#like reanalysis_tdtr_k, year, ndvi_ne, reanalysis_max_air_temp_k, ndvi_se, station_diur_temp_rng_c, weekofyear, ndvi_nw

dengue_train_sj.drop('city', axis=1, inplace=True)
dengue_train_iq.drop('city', axis=1, inplace=True)

dengue_train_sj.drop('week_start_date', axis=1, inplace=True)
dengue_train_iq.drop('week_start_date', axis=1, inplace=True)

dengue_train_sj.drop('reanalysis_tdtr_k', axis=1, inplace=True)
dengue_train_iq.drop('reanalysis_tdtr_k', axis=1, inplace=True)

dengue_train_sj.drop('ndvi_ne', axis=1, inplace=True)
dengue_train_iq.drop('ndvi_ne', axis=1, inplace=True)

dengue_train_sj.drop('reanalysis_max_air_temp_k', axis=1, inplace=True)
dengue_train_iq.drop('reanalysis_max_air_temp_k', axis=1, inplace=True)

dengue_train_sj.drop('ndvi_se', axis=1, inplace=True)
dengue_train_iq.drop('ndvi_se', axis=1, inplace=True)

dengue_train_sj.drop('station_diur_temp_rng_c', axis=1, inplace=True)
dengue_train_iq.drop('station_diur_temp_rng_c', axis=1, inplace=True)

dengue_train_sj.drop('ndvi_nw', axis=1, inplace=True)
dengue_train_iq.drop('ndvi_nw', axis=1, inplace=True)

dengue_test_sj.drop('city', axis=1, inplace=True)
dengue_test_iq.drop('city', axis=1, inplace=True)

dengue_test_sj.drop('week_start_date', axis=1, inplace=True)
dengue_test_iq.drop('week_start_date', axis=1, inplace=True)

# Remove `week_start_date` string.
dengue_test_sj.drop('reanalysis_tdtr_k', axis=1, inplace=True)
dengue_test_iq.drop('reanalysis_tdtr_k', axis=1, inplace=True)

# dengue_test_sj.drop('year', axis=1, inplace=True)
# dengue_test_iq.drop('year', axis=1, inplace=True)

dengue_test_sj.drop('ndvi_ne', axis=1, inplace=True)
dengue_test_iq.drop('ndvi_ne', axis=1, inplace=True)

dengue_test_sj.drop('reanalysis_max_air_temp_k', axis=1, inplace=True)
dengue_test_iq.drop('reanalysis_max_air_temp_k', axis=1, inplace=True)

dengue_test_sj.drop('ndvi_se', axis=1, inplace=True)
dengue_test_iq.drop('ndvi_se', axis=1, inplace=True)

dengue_test_sj.drop('station_diur_temp_rng_c', axis=1, inplace=True)
dengue_test_iq.drop('station_diur_temp_rng_c', axis=1, inplace=True)

# dengue_test_sj.drop('weekofyear', axis=1, inplace=True)
# dengue_test_iq.drop('weekofyear', axis=1, inplace=True)

dengue_test_sj.drop('ndvi_nw', axis=1, inplace=True)
dengue_test_iq.drop('ndvi_nw', axis=1, inplace=True)

from sklearn import preprocessing

def fillNans(df_train, df_test):

  idf_train = df_train.copy()
  idf_train.drop('total_cases', axis=1, inplace=True)
  idf_train['source'] = 'Train'

  idf_test = df_test.copy()
  idf_test['source'] = 'Test'


  idf_sum = pd.concat([idf_train, idf_test]).reset_index(drop=True)

  idf_sum_in = idf_sum.copy()
  idf_sum_in.drop('source', axis=1, inplace=True)

  scaler = preprocessing.MinMaxScaler()
  x = idf_sum_in.values #returns a numpy array
  min_max_scaler = preprocessing.MinMaxScaler()
  x_scaled = min_max_scaler.fit_transform(x)
  idf = pd.DataFrame(x_scaled)
  idf.columns = idf_sum_in.columns
  idf.index = idf_sum_in.index
  idf['source'] = idf_sum['source']
 
  idf_out_train = idf[idf.source == 'Train'].reset_index(drop=True).copy()
  idf_out_train.drop('source', axis=1, inplace=True)
  idf_out_train['total_cases'] = df_train['total_cases'].reset_index(drop=True)
  idf_out_test = idf[idf.source == 'Test'].reset_index(drop=True).copy()
  idf_out_test.drop('source', axis=1, inplace=True)
  return idf_out_train, idf_out_test


dengue_train_sj_n, dengue_test_sj_n = fillNans(dengue_train_sj, dengue_test_sj)
dengue_train_iq_n, dengue_test_iq_n = fillNans(dengue_train_iq, dengue_test_iq)

dengue_train_sj_n.fillna(method='ffill', inplace=True)
dengue_train_iq_n.fillna(method='ffill', inplace=True)

dengue_test_sj_n.fillna(method='ffill', inplace=True)
dengue_test_iq_n.fillna(method='ffill', inplace=True)

dengue_train_sj_n.head()

from sklearn.impute import KNNImputer

def fillNans(df_train, df_test):

  idf_train = df_train.copy()
  idf_train.drop('total_cases', axis=1, inplace=True)
  idf_train['source'] = 'Train'

  idf_test = df_test.copy()
  idf_test['source'] = 'Test'


  idf_sum = pd.concat([idf_train, idf_test]).reset_index(drop=True)

  idf_sum_in = idf_sum.copy()
  idf_sum_in.drop('source', axis=1, inplace=True)

  imputer = KNNImputer(n_neighbors=5, weights="distance")
  idf = pd.DataFrame(imputer.fit_transform(idf_sum_in))
  idf.columns = idf_sum_in.columns
  idf.index = idf_sum_in.index
  idf['source'] = idf_sum['source']
 
  idf_out_train = idf[idf.source == 'Train'].reset_index(drop=True).copy()
  idf_out_train.drop('source', axis=1, inplace=True)
  idf_out_train['total_cases'] = df_train['total_cases'].reset_index(drop=True)
  idf_out_test = idf[idf.source == 'Test'].reset_index(drop=True).copy()
  idf_out_test.drop('source', axis=1, inplace=True)
  return idf_out_train, idf_out_test

dengue_train_sj_n, dengue_test_sj_n = fillNans(dengue_train_sj, dengue_test_sj)
dengue_train_iq_n, dengue_test_iq_n = fillNans(dengue_train_iq, dengue_test_iq)

dengue_train_iq

def MultipleRandFrorest(idf, K=50,  nest=100, rnd=20):

  df = idf.copy()
  train_Y = df.total_cases
  df.drop('total_cases', axis=1, inplace=True)
  

  models = {}
  acc = []

  for i in range(0, K):
    newData = np.random.randint(df.shape[0], size = df.shape[0])
    newTest = np.delete(np.arange(0,df.shape[0]),np.unique(newData))

    data_X = df.iloc[newData,:].reset_index(drop=True)
    data_Y = train_Y.iloc[newData].reset_index(drop=True)

    models[i] = RandomForestRegressor(n_estimators=nest, random_state=rnd)
    models[i].fit(data_X, data_Y)

    tst_X = df.iloc[newTest,:].reset_index(drop=True)
    tst_Y = train_Y.iloc[newTest].reset_index(drop=True)

    predictions = models[i].predict(tst_X).astype(int)
    acc.append(eval_measures.meanabs(predictions,tst_Y))

  acc = np.asarray(acc)
  out_models = {}
  out_ACC = []
  for i in range(0,round(K)):

    indx = acc.argmin()
    out_ACC.append(acc[indx])
    acc[indx] = 100
    out_models[i] = models[indx]

  out_ACC = np.asarray(out_ACC)
  print(str(out_ACC.mean()))

  return out_models

def optimizeModel(df):

  n_estimate = np.arange(10, 100, 5, dtype=np.float64)  
  rand_state = np.arange(5, 100, 5, dtype=np.float64)                 
  best_estimate = []
  best_rand_state = []
  best_score = 1000
  eee = 10 ** -6

  for n in n_estimate:
      for r in rand_state:
        nest = int(n)
        rnd = int(r)
        MultipleModels, ACC = MultipleRandFrorest(df, 5, nest, rnd)

        if ACC < best_score + eee:
          best_score = ACC
          best_estimate = nest
          best_rand_state = rnd
          print(best_estimate)
          print(best_rand_state)
  
  MultipleModels, ACC = MultipleRandFrorest(df, 5, best_estimate, best_rand_state)
  print(str(ACC))
  
  return MultipleModels

# 40 95 sj
# 90 70 iq

sj_MultipleModels = MultipleRandFrorest(dengue_train_sj_n, 50, 100, 8)
iq_MultipleModels = MultipleRandFrorest(dengue_train_iq_n, 50, 100, 8)

def ClassMultpRandForest(models, idf):

  df = idf.copy()
  
  output = pd.DataFrame()
  for i in range(0,len(models)):
    output.insert(i, str(i), models[i].predict(df).astype(int))

  output['out'] = output.mode(axis=1).iloc[:,0]
  return output['out'].to_numpy().astype(int) 

sj_predictions = ClassMultpRandForest(sj_MultipleModels, dengue_test_sj_n)
iq_predictions = ClassMultpRandForest(iq_MultipleModels, dengue_test_iq_n)

sj_out = pd.DataFrame()
sj_out['year'] = dengue_test_sj['year'].astype(int)
sj_out['weekofyear'] = dengue_test_sj['weekofyear'].astype(int)
sj_out['city'] = 'sj'
sj_out['total_cases'] = sj_predictions.astype(int)
sj_out = sj_out[['city', 'year', 'weekofyear', 'total_cases']]

iq_out = pd.DataFrame()
iq_out['year'] = dengue_test_iq['year'].astype(int)
iq_out['weekofyear'] = dengue_test_iq['weekofyear'].astype(int)
iq_out['city'] = 'iq'
iq_out['total_cases'] = iq_predictions.astype(int)
iq_out = iq_out[['city', 'year', 'weekofyear', 'total_cases']]

DF = pd.concat([sj_out, iq_out])
DF.to_csv("Submission_out.csv", index=False)

DF