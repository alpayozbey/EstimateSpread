# -*- coding: utf-8 -*-
"""Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lw_TIx6QQ30uuBbQWV_jDZmRIu9iEnLL
"""

import re
import time
import datetime 
import operator
import numpy as np
import pandas as pd 
import collections
import unicodedata
import collections
import seaborn as sns
import collections
import matplotlib.pylab as pylab
import matplotlib.pyplot as plt

from tqdm import tqdm
from collections import Counter
from datetime import datetime, date, timedelta
from IPython.display import Image
from sklearn.ensemble import RandomForestRegressor

D_train_features = pd.read_csv('dengue_features_train.csv')
D_test = pd.read_csv('dengue_features_test.csv')
D_train_labels = pd.read_csv('dengue_labels_train.csv')

D_train = pd.merge(D_train_features, D_train_labels, on=['city', 'year', 'weekofyear'])

D_train_sj = D_train[D_train.city == 'sj'].copy()
D_train_iq = D_train[D_train.city == 'iq'].copy()

D_test_sj = D_test[D_test.city == 'sj'].copy()
D_test_iq = D_test[D_test.city == 'iq'].copy()

D_train_sj.fillna(method='ffill', inplace=True)
D_train_iq.fillna(method='ffill', inplace=True)

D_test_sj.fillna(method='ffill', inplace=True)
D_test_iq.fillna(method='ffill', inplace=True)

D_train_sj.drop('city', axis=1, inplace=True)
D_train_sj.drop('week_start_date', axis=1, inplace=True)

D_test_sj.drop('city', axis=1, inplace=True)
D_test_sj.drop('week_start_date', axis=1, inplace=True)

D_train_iq.drop('city', axis=1, inplace=True)
D_train_iq.drop('week_start_date', axis=1, inplace=True)

D_test_iq.drop('city', axis=1, inplace=True)
D_test_iq.drop('week_start_date', axis=1, inplace=True)

Features = list(D_train_sj.keys())[:-1]
A = list(itertools.combinations(Features, 2))

D_train_sj[list(A[5])]

#Data reading
#Load train and test csv file
dengue_features_train = pd.read_csv('dengue_features_train.csv')
dengue_features_test = pd.read_csv('dengue_features_test.csv')
dengue_labels_train = pd.read_csv('dengue_labels_train.csv')

#Display the train data
columns = dengue_features_train.columns
ModelFormula = columns[-1] + ' ~ '
for c in columns[:-2]:
  ModelFormula += str(c) + ' + '

ModelFormula += str(columns[-2])

ModelFormula

dengue_features_train.describe()

#row count 
dengue_features_train.shape

dengue_labels_train

#Let's fusion the 2 dataframes:
#Merging the Train dataframe with the labels data frame 

dengue_train = pd.merge(dengue_labels_train, dengue_features_train, on=['city','year','weekofyear'])

dengue_train

dengue_train.shape

#Check duplicate rows
np.sum(dengue_train.duplicated())

dengue_features_test

dengue_features_test.shape

#Check duplicate rows
np.sum(dengue_features_test.duplicated())

#Lets check the total cases of Dengue in the city's
sns.set(style="ticks", palette="colorblind")
g = sns.FacetGrid(dengue_train, col="city",aspect=2)  
g.map(sns.distplot, "total_cases") 
axes = g.axes
axes[0,0].set_ylim(0,0.090)
axes[0,1].set_ylim(0,0.090)

#Lets check the station_precip_mm in the city's
sns.set(style="ticks", palette="colorblind")
g = sns.FacetGrid(dengue_train, col="city",aspect=2)  
g.map(sns.distplot, "station_precip_mm")

#Lets check the station_max_temp_c in the city's
sns.set(style="ticks", palette="colorblind")
g = sns.FacetGrid(dengue_train, col="city",aspect=2)  
g.map(sns.distplot, "station_max_temp_c")

#Lets check the station_min_temp_c in the city's
sns.set(style="ticks", palette="colorblind")
g = sns.FacetGrid(dengue_train, col="city",aspect=2)  
g.map(sns.distplot, "station_min_temp_c")

#Lets check the station_avg_temp_c in the city's
sns.set(style="ticks", palette="colorblind")
g = sns.FacetGrid(dengue_train, col="city",aspect=2)  
g.map(sns.distplot, "station_avg_temp_c")

dengue_train.groupby('city').mean().total_cases

sns.set(style="ticks", palette="colorblind")
fig = sns.FacetGrid(dengue_train, hue='city', aspect=4) 
fig.map(sns.pointplot,'weekofyear','total_cases')
max_x = dengue_train.weekofyear.max()
min_x = dengue_train.weekofyear.min()
fig.set(xlim=(min_x,max_x))
fig.set(ylim=(0, 80))
fig.add_legend()
fig.fig.suptitle("Evolution of dengue disease through time")

#Finding the number of NUll or NaN values in the dataframe
# dengue_train.isnull().sum()

# dengue_features_test.isnull().sum()

#Let see the count of unique city's available 

# dengue_train.city.value_counts()

#Let see the count of unique city's available 

# dengue_features_test.city.value_counts()

dengue_train_sj = dengue_train[dengue_train.city == 'sj'].copy()

dengue_train_iq = dengue_train[dengue_train.city == 'iq'].copy()

dengue_test_sj = dengue_features_test[dengue_features_test.city == 'sj'].copy()
dengue_test_iq = dengue_features_test[dengue_features_test.city == 'iq'].copy()

# dengue_train_sj

# dengue_train_iq

# dengue_train_sj.isnull().sum()

# dengue_train_sj[dengue_train_sj.isnull().any(axis=1)]

def ReplaceNANs(DF):
  hasNANs = []
  for i in DF.isnull().sum().keys():
    if DF.isnull().sum()[i] > 0:
      hasNANs.append(i)


  for n in hasNANs:
    DF[n] = DF[n].fillna(DF.groupby('year')[n].transform('mean'))
  return DF
# dengue_train_sj = ReplaceNANs(dengue_train_sj)
# dengue_train_iq = ReplaceNANs(dengue_train_iq)
# dengue_test_sj = ReplaceNANs(dengue_test_sj)
# dengue_test_iq = ReplaceNANs(dengue_test_iq)

dengue_train_sj.fillna(method='ffill', inplace=True)
dengue_train_iq.fillna(method='ffill', inplace=True)
dengue_test_sj.fillna(method='ffill', inplace=True)
dengue_test_iq.fillna(method='ffill', inplace=True)

dengue_train_sj.isnull().sum()

print('San Juan')
print('mean: ', dengue_train_sj.total_cases.mean())
print('var :', dengue_train_sj.total_cases.var())

print('\nIquitos')
print('mean: ', dengue_train_iq.total_cases.mean())
print('var :', dengue_train_iq.total_cases.var())

#Which inputs strongly correlate with total_cases

# compute the correlations
sj_correlations = dengue_train_sj.corr()
iq_correlations = dengue_train_iq.corr()

import matplotlib.cm as cm
from matplotlib import cm
cmap = cmap=sns.diverging_palette(5, 250, as_cmap=True)

def magnify():
    return [dict(selector="th",
                 props=[("font-size", "7pt")]),
            dict(selector="td",
                 props=[('padding', "0em 0em")]),
            dict(selector="th:hover",
                 props=[("font-size", "12pt")]),
            dict(selector="tr:hover td:hover",
                 props=[('max-width', '200px'),
                        ('font-size', '12pt')])
]

sj_correlations.style.background_gradient(cmap, axis=1)\
    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\
    .set_caption("Hover to magify")\
    .set_precision(2)\
    .set_table_styles(magnify())

sns.heatmap(sj_correlations)

cmap = cmap=sns.diverging_palette(5, 250, as_cmap=True)
def magnify():
    return [dict(selector="th",
                 props=[("font-size", "7pt")]),
            dict(selector="td",
                 props=[('padding', "0em 0em")]),
            dict(selector="th:hover",
                 props=[("font-size", "12pt")]),
            dict(selector="tr:hover td:hover",
                 props=[('max-width', '200px'),
                        ('font-size', '12pt')])
]

iq_correlations.style.background_gradient(cmap, axis=1)\
    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\
    .set_caption("Hover to magify")\
    .set_precision(2)\
    .set_table_styles(magnify())

# San Juan
(sj_correlations
     .total_cases
     .drop('total_cases') # don't compare with myself
     .sort_values(ascending=False)
     .plot
     .barh()
)
sns.set(style="ticks", palette="colorblind")

sns.set(style="ticks", palette="colorblind")
# iq
(iq_correlations
     .total_cases
     .drop('total_cases') # don't compare with myself
     .sort_values(ascending=False)
     .plot
     .barh()
)

#A few bad variables
#we can drop the columns with negative corellation in both 
#like reanalysis_tdtr_k, year, ndvi_ne, reanalysis_max_air_temp_k, ndvi_se, station_diur_temp_rng_c, weekofyear, ndvi_nw

# Remove `week_start_date` string.


# dengue_train_sj.drop('year', axis=1, inplace=True)
# dengue_train_iq.drop('year', axis=1, inplace=True)
# 8 adet
dengue_train_sj.drop('ndvi_se', axis=1, inplace=True)
dengue_train_sj.drop('precipitation_amt_mm', axis=1, inplace=True)
dengue_train_sj.drop('reanalysis_air_temp_k', axis=1, inplace=True)
dengue_train_sj.drop('reanalysis_avg_temp_k', axis=1, inplace=True)
dengue_train_sj.drop('reanalysis_dew_point_temp_k', axis=1, inplace=True)
dengue_train_sj.drop('reanalysis_max_air_temp_k', axis=1, inplace=True)
dengue_train_sj.drop('reanalysis_min_air_temp_k', axis=1, inplace=True)
dengue_train_sj.drop('reanalysis_tdtr_k', axis=1, inplace=True)
# dengue_train_sj.drop('ndvi_ne', axis=1, inplace=True)
# dengue_train_sj.drop('station_diur_temp_rng_c', axis=1, inplace=True)
# dengue_train_sj.drop('ndvi_nw', axis=1, inplace=True)

# 5 adet
dengue_train_iq.drop('ndvi_ne', axis=1, inplace=True)
dengue_train_iq.drop('precipitation_amt_mm', axis=1, inplace=True)
dengue_train_iq.drop('reanalysis_air_temp_k', axis=1, inplace=True)
dengue_train_iq.drop('reanalysis_tdtr_k', axis=1, inplace=True)
dengue_train_iq.drop('reanalysis_dew_point_temp_k', axis=1, inplace=True)
# dengue_train_iq.drop('station_diur_temp_rng_c', axis=1, inplace=True)
# dengue_train_iq.drop('ndvi_nw', axis=1, inplace=True)
# dengue_train_iq.drop('ndvi_se', axis=1, inplace=True)
# dengue_train_iq.drop('reanalysis_max_air_temp_k', axis=1, inplace=True)

dengue_test_sj.drop('ndvi_se', axis=1, inplace=True)
dengue_test_sj.drop('precipitation_amt_mm', axis=1, inplace=True)
dengue_test_sj.drop('reanalysis_air_temp_k', axis=1, inplace=True)
dengue_test_sj.drop('reanalysis_avg_temp_k', axis=1, inplace=True)
dengue_test_sj.drop('reanalysis_dew_point_temp_k', axis=1, inplace=True)
dengue_test_sj.drop('reanalysis_max_air_temp_k', axis=1, inplace=True)
dengue_test_sj.drop('reanalysis_min_air_temp_k', axis=1, inplace=True)
dengue_test_sj.drop('reanalysis_tdtr_k', axis=1, inplace=True)
dengue_test_sj.drop('ndvi_ne', axis=1, inplace=True)
dengue_test_sj.drop('station_diur_temp_rng_c', axis=1, inplace=True)
dengue_test_sj.drop('ndvi_nw', axis=1, inplace=True)

dengue_test_iq.drop('ndvi_se', axis=1, inplace=True)
dengue_test_iq.drop('reanalysis_max_air_temp_k', axis=1, inplace=True)
dengue_test_iq.drop('ndvi_ne', axis=1, inplace=True)
dengue_test_iq.drop('precipitation_amt_mm', axis=1, inplace=True)
dengue_test_iq.drop('reanalysis_air_temp_k', axis=1, inplace=True)
dengue_test_iq.drop('reanalysis_tdtr_k', axis=1, inplace=True)
dengue_test_iq.drop('reanalysis_dew_point_temp_k', axis=1, inplace=True)
dengue_test_iq.drop('station_diur_temp_rng_c', axis=1, inplace=True)
dengue_test_iq.drop('ndvi_nw', axis=1, inplace=True)

dengue_train_sj.describe()

#Split it up!
#Since this is a timeseries model, we'll use a strict-future holdout set when we are splitting our train set and our test set.
#We'll keep around three quarters of the original data for training and use the rest to test. 
#We'll do this separately for our San Juan model and for our Iquitos model.

print('San Juan')
print(dengue_train_sj.shape)

print('\nIquitos')
print(dengue_train_iq.shape)

print('San Juan')
print(dengue_test_sj.shape)

print('\nIquitos')
print(dengue_test_iq.shape)

sj_train_subtrain = dengue_train_sj.head(800)
sj_train_subtest = dengue_train_sj.tail(dengue_train_sj.shape[0] - 800)

iq_train_subtrain = dengue_train_iq.head(400)
iq_train_subtest = dengue_train_iq.tail(dengue_train_iq.shape[0] - 400)

sj_train_subtrain.head(1)

import itertools

Features = list(DATA.keys())[:-1]
list(itertools.combinations(Features, 1))

from statsmodels.tools import eval_measures
import statsmodels.formula.api as smf

from sklearn.model_selection import train_test_split
import statsmodels.api as sm


from statsmodels.tools import eval_measures
import statsmodels.formula.api as smf

def get_best_model(train, test):
    # Step 1: specify the form of the model
    model_formula = "total_cases ~ 1 + " \
                    "reanalysis_specific_humidity_g_per_kg + " \
                    "reanalysis_dew_point_temp_k + " \
                    "reanalysis_min_air_temp_k + " \
                    "station_min_temp_c + " \
                    "station_max_temp_c + " \
                    "station_avg_temp_c"
                    
        
    
    grid = 10 ** np.arange(-1, -10, -0.2, dtype=np.float64)
                    
    best_alpha = []
    best_score = 1000
    eee = 10 ** -10    
    # Step 2: Find the best hyper parameter, alpha
    for alpha in grid:
        model = smf.glm(formula=model_formula,
                        data=train,
                        family=sm.families.NegativeBinomial(alpha=alpha))

        results = model.fit()
        predictions = results.predict(test).astype(int)
        score = eval_measures.meanabs(predictions, test.total_cases)

        if score + eee < best_score:
            best_alpha = alpha
            best_score = score

    print('best alpha = ', best_alpha)
    print('best score = ', best_score)
            
    # Step 3: refit on entire dataset
    full_dataset = pd.concat([train, test])
    model = smf.glm(formula=model_formula,
                    data=full_dataset,
                    family=sm.families.NegativeBinomial(alpha=best_alpha))

    fitted_model = model.fit()
    return fitted_model
    
sj_best_model = get_best_model(sj_train_subtrain, sj_train_subtest)
iq_best_model = get_best_model(iq_train_subtrain, iq_train_subtest)

import itertools

def OptParameters(parameter2try):
  ll = parameter2try[list(parameter2try.keys())[0]]
  for k in list(parameter2try.keys())[1:]:
    ll = itertools.product(ll, parameter2try[k])
  return list(ll)

L = OptParameters({'val1' : list(range(0,100,5)), 'val2': list(range(0,30,1)), 'val3' : list(range(0,30,1))})
print(L)


for l in L:
  params = []
  for i in range(0,numberof)
    params.append(l[1])
    l = l[0]
    params =

DICT = {'val1' : list(range(0,100,5)), 'val2': list(range(0,30,1))}
K = list(DICT.keys())[0]
print(DICT[K])

ff = expand.grid(aa = range(0,1,0.1), bb = range(0,1,0.1))

model_formula = "total_cases ~ 1 + " \
                    "reanalysis_specific_humidity_g_per_kg + " \
                    "reanalysis_dew_point_temp_k + " \
                    "reanalysis_min_air_temp_k + " \
                    "station_min_temp_c + " \
                    "station_max_temp_c + " \
                    "station_avg_temp_c"

model_formula

from statsmodels.tools import eval_measures
import statsmodels.formula.api as smf

from sklearn.model_selection import train_test_split
import statsmodels.api as sm


from statsmodels.tools import eval_measures
import statsmodels.formula.api as smf

def get_best_model(train, test):
    # Step 1: specify the form of the model
    model_formula = "total_cases ~ 1 + " \
                    "reanalysis_specific_humidity_g_per_kg + " \
                    "reanalysis_dew_point_temp_k + " \
                    "reanalysis_min_air_temp_k + " \
                    "station_min_temp_c + " \
                    "station_max_temp_c + " \
                    "station_avg_temp_c"

    full_dataset = pd.concat([train, test])
    model = smf.glm(formula=model_formula,
                    data=full_dataset,
                    family=sm.families.Gaussian())


    fitted_model = model.fit()
    acc = eval_measures.rmse(sj_best_model.predict(full_dataset).astype(int), full_dataset.total_cases)

    return fitted_model, acc

sj_best_model, RMSE_SJ = get_best_model(sj_train_subtrain, sj_train_subtest)
iq_best_model, RMSE_IQ = get_best_model(iq_train_subtrain, iq_train_subtest)
print('SJ: %f, IQ: %f' %(RMSE_SJ, RMSE_IQ))

def bestrandfrorest(train, test):


    n_estimate = np.arange(2, 100, 2, dtype=np.float64)  
    rand_state = np.arange(2, 100, 2, dtype=np.float64)                 
    best_estimate = []
    best_rand_state = []
    best_score = 1000
    eee = 10 ** -6

    train_X = train.copy()
    train_Y = train_X.total_cases
    train_X.drop('total_cases', axis=1, inplace=True)
    train_X.drop('city', axis=1, inplace=True)
    train_X.drop('week_start_date', axis=1, inplace=True)

    test_X = test.copy()
    test_Y = test_X.total_cases
    test_X.drop('total_cases', axis=1, inplace=True)
    test_X.drop('city', axis=1, inplace=True)
    test_X.drop('week_start_date', axis=1, inplace=True)

    for n in n_estimate:
      for r in rand_state:
        nest = int(n)
        rnd = int(r)

        randForestModel = RandomForestRegressor(n_estimators=nest, random_state=rnd)
        randForestModel.fit(train_X, train_Y)

        predictions = randForestModel.predict(test_X).astype(int)
        acc = eval_measures.meanabs(predictions, test_Y)

        if acc < best_score + eee:
          best_score = acc
          best_estimate = nest
          best_rand_state = rnd
      
      print(n)

    print(best_estimate)
    print(best_rand_state)
    randForestModel = RandomForestRegressor(n_estimators=best_estimate, random_state=best_rand_state)
    randForestModel.fit(train_X, train_Y)
    predictions = randForestModel.predict(test_X).astype(int)
    acc = eval_measures.meanabs(predictions, test_Y)
    return randForestModel, acc



sj_best_model, RMSE_SJ = bestrandfrorest(sj_train_subtrain, sj_train_subtest)
iq_best_model, RMSE_IQ = bestrandfrorest(iq_train_subtrain, iq_train_subtest)
print('SJ: %f, IQ: %f' %(RMSE_SJ, RMSE_IQ))

figs, axes = plt.subplots(nrows=2, ncols=1)

full_dataset = pd.concat([sj_train_subtrain, sj_train_subtest])
full_dataset.drop('total_cases', axis=1, inplace=True)
full_dataset.drop('city', axis=1, inplace=True)
full_dataset.drop('week_start_date', axis=1, inplace=True)


# plot sj
dengue_train_sj['fitted'] = sj_best_model.predict(full_dataset).astype(int)
dengue_train_sj.fitted.plot(ax=axes[0], label="Predictions")
dengue_train_sj.total_cases.plot(ax=axes[0], label="Actual")

# plot iq

full_dataset = pd.concat([iq_train_subtrain, iq_train_subtest])
full_dataset.drop('total_cases', axis=1, inplace=True)
full_dataset.drop('city', axis=1, inplace=True)
full_dataset.drop('week_start_date', axis=1, inplace=True)

dengue_train_iq['fitted'] = iq_best_model.predict(full_dataset).astype(int)
dengue_train_iq.fitted.plot(ax=axes[1], label="Predictions")
dengue_train_iq.total_cases.plot(ax=axes[1], label="Actual")

plt.suptitle("Dengue Predicted Cases vs. Actual Cases")
plt.legend()

dengue_test_sj.drop('city', axis=1, inplace=True)
dengue_test_sj.drop('week_start_date', axis=1, inplace=True)

dengue_test_iq.drop('city', axis=1, inplace=True)
dengue_test_iq.drop('week_start_date', axis=1, inplace=True)

sj_predictions = sj_best_model.predict(dengue_test_sj).astype(int)
iq_predictions = iq_best_model.predict(dengue_test_iq).astype(int)

submission = pd.read_csv("submission_format.csv",
                         index_col=[0, 1, 2])
submission.total_cases = np.concatenate([sj_predictions, iq_predictions])
submission.to_csv("Submission_out.csv")